# üöÄ Classifica√ß√£o de Renda (Adult Income) | XGBoost Otimizado
![Status: Conclu√≠do | F1 Score Otimizado: 0.7219]

## 1. Vis√£o Geral do Projeto

Este projeto consiste na cria√ß√£o de um modelo de Machine Learning de alta performance para classificar a renda anual de indiv√≠duos em duas categorias: **<=50K** ou **>50K** (Alta Renda), utilizando o Adult Income Dataset.

O objetivo principal do desafio foi a **Maximiza√ß√£o da Performance** do modelo, utilizando o **F1 Score** como m√©trica prim√°ria de avalia√ß√£o. O fluxo de trabalho seguiu as melhores pr√°ticas de Engenharia de IA, desde o pr√©-processamento robusto at√© a explicabilidade avan√ßada (XAI).

---

## 2. Metodologia e Pipeline de Engenharia

O projeto utilizou o algoritmo **XGBoost (Extreme Gradient Boosting)**, conhecido por sua efici√™ncia em dados tabulares, aplicando um pipeline rigoroso de prepara√ß√£o de dados.

### üõ†Ô∏è Fases de Engenharia de Atributos

* **Pr√©-Processamento (Obrigat√≥rio):**
    * Tratamento de valores faltantes (NaNs) via imputa√ß√£o por Moda.
    * Controle de *Outliers* (valores extremos) nas vari√°veis num√©ricas via *Capping* por IQR (Intervalo Interquartil).
* **Feature Engineering:**
    * **Codifica√ß√£o de Categorias:** Aplica√ß√£o de One-Hot Encoding (OHE) nas 8 vari√°veis categ√≥ricas.
    * **Dimensionamento:** Uso de StandardScaler nas vari√°veis num√©ricas.
    * **Sele√ß√£o de Atributos:** Treinamento de um *modelo baseline* para identificar e reter apenas as **20 *features* mais importantes**, reduzindo o ru√≠do e combatendo o *overfitting*.
* **Tunning e Otimiza√ß√£o:**
    * Aplica√ß√£o de **Grid Search Cross-Validation** (CV=3) para otimizar os hiperpar√¢metros do XGBoost (`max_depth`, `learning_rate`, `n_estimators`) em busca do melhor F1 Score.
    * **Otimiza√ß√£o do Limiar de Decis√£o:** Ajuste fino do *threshold* de 0.50 para **0.40**, resultando no **melhor equil√≠brio entre Precis√£o e Recall**.

---

## 3. Relat√≥rio de Resultados e Performance Final

O modelo final (`XGBoost Otimizado`) demonstrou uma melhoria significativa no F1 Score em rela√ß√£o ao modelo *baseline*.

### üìà Tabela de Performance (Valida√ß√£o)

| M√©trica | Resultado Baseline (Limiar 0.50) | Resultado Final Otimizado (Limiar 0.40) |
| :---: | :---: | :---: |
| **F1 Score (Destaque)** | 0.7087 | **0.7219** |
| Precision | 0.7639 | 0.6983 |
| Recall | 0.6609 | **0.7488** |

### üîë Hiperpar√¢metros e Ajustes Finais

| Par√¢metro | Valor Otimizado |
| :--- | :--- |
| **Melhor Estimador** | XGBClassifier |
| Learning Rate | 0.1 |
| Max Depth | 5 |
| N Estimators | 200 |
| **Limiar de Decis√£o** | **0.40** (Ajustado) |

---

## 4. Explicabilidade do Modelo (XAI - SHAP)

Conforme requisito do projeto, a ferramenta **SHAP (SHapley Additive exPlanations)** foi utilizada para explicar a contribui√ß√£o de cada atributo para a previs√£o final.

### Principais Insights do Modelo:

O gr√°fico SHAP revelou que os fatores com maior poder preditivo (que mais "empurram" a previs√£o para Renda Alta) s√£o:

1.  **Estado Civil (`Married-civ-spouse`):** √â o fator isolado com o maior impacto positivo na previs√£o de Renda Alta.
2.  **Idade (`num_age`):** Quanto maior a idade, maior a contribui√ß√£o para a previs√£o de Renda Alta.
3.  **Ganho de Capital (`num_capital-gain`):** Alto ganho de capital √© um forte indicador de Tesouro (>50K).

Este resultado valida a robustez do modelo e fornece **transpar√™ncia** sobre suas decis√µes.

---

## 5. Conclus√£o: Desafios e Tomadas de Decis√£o

O sucesso deste projeto n√£o residiu apenas na aplica√ß√£o de um algoritmo robusto (XGBoost), mas principalmente nas **decis√µes estrat√©gicas de Engenharia de IA** tomadas em cada etapa do pipeline:

* **Desafio da Qualidade dos Dados (Outliers):** A principal decis√£o de Engenharia de Dados foi evitar a exclus√£o de *outliers* em colunas cr√≠ticas como `capital-gain`. A exclus√£o teria removido exemplos raros, mas vitais, da classe minorit√°ria (`>50K`). Optamos pelo **Capping via IQR** para controlar a escala sem perder a informa√ß√£o bin√°ria da alta riqueza.
* **Controle de Complexidade (Sele√ß√£o de Atributos):** Ap√≥s o One-Hot Encoding expandir o *dataset* para 108 *features*, foi crucial empregar a **Sele√ß√£o de Atributos** baseada na import√¢ncia do XGBoost. Retivemos apenas as 20 *features* mais preditivas, simplificando o modelo, reduzindo o risco de *overfitting* e acelerando o *tunning*.
* **Otimiza√ß√£o do F1 Score (Limiar de Decis√£o):** O desafio final de maximiza√ß√£o do F1 Score foi superado n√£o apenas pelo *tunning* de hiperpar√¢metros, mas pela otimiza√ß√£o do **Limiar de Decis√£o**. O ajuste do *threshold* padr√£o de 0.50 para **0.40** permitiu que o modelo se tornasse estrategicamente mais agressivo, aumentando o **Recall** (achando mais Renda Alta real) e elevando o F1 Score final para **0.7219**.
* **Atendimento √† Explicabilidade (XAI):** Para abordar o desafio de **interpretabilidade** inerente aos modelos de *ensemble* (XGBoost), aplicamos a t√©cnica **SHAP**. Isso permitiu n√£o s√≥ validar os *insights* do modelo mas tamb√©m cumprir o requisito fundamental de fornecer **transpar√™ncia e justifica√ß√£o** para as decis√µes do modelo.

Este processo demonstra a capacidade de transformar requisitos de neg√≥cio em decis√µes t√©cnicas que garantem o melhor desempenho e a robustez do modelo.
